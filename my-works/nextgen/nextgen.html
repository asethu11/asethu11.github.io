<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <title>NextGen Project</title>
        <meta name="author" content="asethu11">
        <meta name="description" content="Portfolio of Abhishek, a UX Designer based in USA.">
        <meta name="keywords" content="UX, UI, Design, Portfolio, Arizona, USA">
        <meta name="robots" content="index, follow">
        <meta name="theme-color" content="#040404" media="(prefers-color-scheme: dark)">
        <meta name="theme-color" content="#FCFCFC" media="(prefers-color-scheme: light)">

        <script src="../../script.js"></script>
        <script src="../../shared.js"></script>
        <link rel="stylesheet" href="nextgen.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

        <link rel="icon" href="/img/favicon.png" type="image/png">
    </head>
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-98H0ZTK0T4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-98H0ZTK0T4');
</script>

<script>
    function updateImages() {
        const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches;
        document.querySelectorAll(".theme-img").forEach(img => {
            const imageName = img.dataset.name; // Get the image name
            const newSrc = isDarkMode 
                ? `img/dk/${imageName}.png` 
                : `img/lt/${imageName}.png`;

            // Only update if the src has changed (prevents unnecessary reloads)
            if (img.src !== newSrc) {
                img.src = newSrc;
            }
        });
        
        // Update view image links
        document.querySelectorAll(".view-image-link").forEach(link => {
            const imageName = link.dataset.name;
            link.href = isDarkMode 
                ? `img/dk/${imageName}.png` 
                : `img/lt/${imageName}.png`;
        });
    }

    // Ensure images update on theme change
    const darkModeQuery = window.matchMedia("(prefers-color-scheme: dark)");
    darkModeQuery.addEventListener("change", updateImages);

    // Run on page load
    document.addEventListener("DOMContentLoaded", updateImages);
</script>

<body>
    <a href="../../index.html" class="back-button">
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
            <path d="M19 12H5M5 12L12 19M5 12L12 5" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
        </svg>
        Home
    </a>
    
    <div class="article-container">
        <article class="article-body">
            
            <!--Intro Section-->
            <section class="section-1">

                <div class="grey-background-article image-bottom">
                    <figure class="article-figure">
                        <img class="theme-img" src="" data-name="ToolBag_OnSeen" width="100%" alt="NextGen Project Header">
                    </figure>
                </div>

                <h1 class="article-h1">VR Training Platform for Minnesota's Law Enforcement Trainees</h1>

                <p>
                    NextGen Badge is a VR training platform that immerses criminal justice students in realistic, emotionally charged scenarios like traffic stops and domestic disputes. Built in Unreal Engine 5, it creates a safe, repeatable space to test both technical skills and emotional resilience. <br><br>

                    As the Product Designer (UX-focused), I led experience design, supported production, and collaborated closely with developers, 3D artists, and the PM from concept through ongoing development.
                </p>

                <div class="sub-header">
                    <p id="intro-tags1">
                        5-7 min read | June 2024 → June 2025* | Product Designer & TL | VR, Unreal Engine 5, Unity
                    </p>
                </div>

            </section>

            <hr class="intro-hr">

            <section class="section-tldr">
                <h2>TL;DR</h2>
                <p>Traditional police training focuses on procedure, not empathy. The University of Minnesota and Minneapolis PD wanted a way to teach emotional awareness and real-time decision-making through VR.</p>

                <p><b>Solution:</b></p>
                <p>We built NextGen Badge in Unreal Engine 5 for Oculus Quest 3 — an immersive training platform where students navigate high-pressure scenarios like traffic stops and domestic calls. I led UX design, scripted branching dialogues, defined interaction systems, and coordinated production across design, art, and development.</p>

                <p><b>Result:</b></p>
                <p>Delivered two fully interactive, performance-optimized VR scenarios with realistic dialogue, spatial audio, and emotional pacing. Early testing showed stronger engagement and improved decision awareness, guiding expansion for broader rollout in 2025.</p>
            </section>
            
            <hr class="intro-hr">

            <figure class="article-figure">
                <video class="article-video" controls autoplay muted loop>
                    <source src="vid/UE5 VR Simulation Snippet - Raycast Observation & UI Interactions in Police Training (1).mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <figcaption>Gameplay Snippet: Raycast Observation & UI Interactions in Police Training | Turn on sound for dialogues.</figcaption>
            </figure>

            <!--Main Content Sections-->
            <section class="section-2">

                <h2 class="article-h2">Context</h2>
                <p>Traditional police training focuses heavily on procedures but often overlooks emotional intelligence. The University of Minnesota and Minneapolis PD partnered with us to explore how VR could help bridge that gap.</p>
                
                <p>I worked closely with the PM, client, 3D artists, and Unreal developers to:</p>
                
                <ul>
                    <li>Design the end-to-end in-headset experience.</li>
                    <li>Define UI systems for gaze, triggers, and onboarding.</li>
                    <li>Write branching dialogue scripts and produce storyboards.</li>
                    <li>Maintain design documentation and asset lists.</li>
                    <li>Direct visual tone and audio direction with the art and sound teams.</li>
                </ul>

                <p><b>Team:</b> Me (Product Design) · 2 Unreal Developers · 2 3D Artists · 1 Sound Designer</p>


            
                <h2 class="article-h2">Approach</h2>
            
                <h3 class="article-h3">Scenario Design</h3>
                <p>We designed two key simulations — <b>Traffic Stop</b> and <b>Domestic Call</b> — built around empathy and decision-making under pressure. Working with law-enforcement SMEs ensured realism, accuracy, and authentic pacing.</p>

                <figure class="article-figure">
                    <img class="theme-img" src="" data-name="Visual Design - TS" width="100%" alt="Visual Design - Type Scale">
                    <figcaption>Planning of the Domestic Call scenario · <a href="#" class="view-image-link" data-name="Visual Design - TS" target="_blank">view img↗</a></figcaption>
                </figure>

                <figure class="article-figure">
                    <img class="theme-img" src="" data-name="Ideation UserFlow-Interactions" width="100%" alt="Ideation, User Flow, and Interactions">
                    <figcaption>Ideation process, user flows, and interaction patterns · <a href="#" class="view-image-link" data-name="Ideation UserFlow-Interactions" target="_blank">view img↗</a></figcaption>
                </figure>

                <h3 class="article-h3">Script & Storyboarding</h3>
                <p>I turned branching dialogue scripts into interactive storyboards and Unreal Blueprints. Dialogue was paced to create emotional tension and reflection moments for trainees.</p>
                <p>Early prototypes used <b>AI-generated voice acting (ElevenLabs)</b> for iteration, later replaced by professional recordings integrated into the same Blueprint system.</p>
            
                <h3 class="article-h3">Interaction & UI</h3>
                <p>The interface was intentionally minimal. Users interacted through subtle gaze and ray-cast cues rather than menus, keeping focus on body language and tone.</p>
                
                <p>Haptics were tied to feedback moments like prompts, item pickups, and emotional beats.</p>

                <figure class="article-figure">
                    <img class="theme-img" src="" data-name="Interactions" width="100%" alt="Interaction Design">
                    <figcaption>Planning of the Interaction System · <a href="#" class="view-image-link" data-name="Interactions" target="_blank">view img↗</a></figcaption>
                </figure>

                <figure class="article-figure">
                    <img class="theme-img" src="" data-name="Screenshot 2025-11-02 at 8.14.23 PM" width="100%" alt="Additional Screen">
                    <figcaption>Early wireframes and Scenario Selector screens · <a href="#" class="view-image-link" data-name="Screenshot 2025-11-02 at 8.14.23 PM" target="_blank">view img↗</a></figcaption>
                </figure>


            
                <h3 class="article-h3">Visual Direction</h3>
                <p>Every detail referenced Minnesota: lighting, weather, and props grounded the experience in a familiar but neutral environment.</p>

                <figure class="article-figure">
                    <img class="theme-img" src="" data-name="Visual Design - DC" width="100%" alt="Visual Design - Design Components">
                    <figcaption>Planning of the Traffic Stop scenario · <a href="#" class="view-image-link" data-name="Visual Design - DC" target="_blank">view img↗</a></figcaption>
                </figure>
                <figure class="article-figure">
                    <img class="theme-img" src="" data-name="early-figma" width="100%" alt="Ideation, User Flow, and Interactions">
                    <figcaption>Wireframes and Figma designs · <a href="#" class="view-image-link" data-name="early-figma" target="_blank">view img↗</a></figcaption>
                </figure>

            
                <h3 class="article-h3">Spatial Audio & Haptics</h3>
                <p>Spatialized dialogue, ambient soundtracks, and tactile cues deepened immersion. For instance, a buzzing phone or a raised voice triggered matching vibrations and localized sound.</p>

                <figure class="article-figure">
                    <img class="theme-img" src="" data-name="Screenshot 2025-08-18 at 1.34.51 PM" width="100%" alt="Feature Screenshot">
                    <figcaption>Dialogue and Spatial Audio implementation · <a href="#" class="view-image-link" data-name="Screenshot 2025-08-18 at 1.34.51 PM" target="_blank">view img↗</a></figcaption>
                </figure>
            
                <h2 class="article-h2">Core Systems</h2>
            
                <h3 class="article-h3">VR Pawn & Player Interaction</h3>
                <p>Custom VR Pawn using Meta XR plugin for Quest 3, with ray-cast visual interactions and alerts.</p>

                <figure class="article-figure">
                    <img class="theme-img" src="" data-name="image" width="100%" alt="Project Image">
                    <figcaption>Custom structured VR Pawn using Meta XR plugin for Quest 3. · <a href="#" class="view-image-link" data-name="image" target="_blank">view img↗</a></figcaption>
                </figure>
        
                <h3 class="article-h3">NPC System + Randomizer</h3> 
                <p>A Blueprint-based MetaHuman randomizer that shuffles NPC bodies, outfits, and voices for scenario variety.</p>
            
                <h3 class="article-h3">Dialogue System</h3>
                <p>Data Table–driven Blueprint logic linking each line to a voice clip, animation cue, and event trigger.</p>
            
                <h3 class="article-h3">Spatial Audio & Haptics</h3>
                <ul>
                    <li>Implemented Spatial Audio in UE using attenuation volumes.</li>
                    <li>Scenario specific soundtracks, Dialogue pinned to NPCs, Subtitles in VR.</li>
                    <li>SFX e.g., rumble during NPC yelling, buzzing phone call, knock vibration.</li>
                </ul>
            


                <h3 class="article-h3">Lighting & Sequencer</h3>
                <p>Combined Lumen reflections with baked lighting for performance. Sequencer timelines handled cutscenes, ambient loops, and branching outcomes.</p>

                <figure class="article-figure">
                    <img class="theme-img" src="" data-name="Screenshot 2025-08-18 at 1.32.45 PM" width="100%" alt="Feature Screenshot">
                    <figcaption>Built Sequencer timelines for cutscenes and ambient NPC animation loops. · <a href="#" class="view-image-link" data-name="Screenshot 2025-08-18 at 1.32.45 PM" target="_blank">view img↗</a></figcaption>
                </figure>
                <figure class="article-figure">
                    <img class="theme-img" src="" data-name="ToolBag_Default" width="100%" alt="ToolBag Default State">
                    <figcaption>Ray-casting based Visual Cues and Alerts · <a href="#" class="view-image-link" data-name="ToolBag_Default" target="_blank">view img↗</a></figcaption>
                </figure>
            
                <h2 class="article-h2">Production Systems</h2>
                <p>We used <b>Notion</b> for sprints, <b>Figma</b> for UX and visual design, and <b>Sheets</b> for asset tracking. Builds and retired assets were organized in <b>Dropbox</b>, while all client deliverables were managed in <b>Google Drive</b>.</p>
                
                <p>Milestones were mapped across 2024–2025, coordinating with developers, testers, and voice actors. I worked closely with PM to understand scope changes, and client feedback I ensured sensitive content aligned with cultural and ethical standards.</p>
                
                <p>Regular syncs with the university and PD stakeholders kept feedback loops fast and transparent.</p>

            
                <h2 class="article-h2">Outcome</h2>
                <p>By mid-2025, <b>NextGen Badge</b> evolved into a functioning VR prototype used by the University of Minnesota's criminal justice program for early testing. Trainees reported that the simulation felt</p>

                <figure class="article-figure">
                    <img class="theme-img" src="" data-name="ToolBag_OnSeen" width="100%" alt="ToolBag OnSeen State">
                    <figcaption>Gameplay Screenshot · <a href="#" class="view-image-link" data-name="ToolBag_OnSeen" target="_blank">view img↗</a></figcaption>
                </figure>
                <figure class="article-figure">
                    <img class="theme-img" src="" data-name="Screenshot 2025-08-18 at 11.51.56 AM" width="100%" alt="Key Feature Screenshot">
                    <figcaption>Gameplay Screenshot · <a href="#" class="view-image-link" data-name="Screenshot 2025-08-18 at 11.51.56 AM" target="_blank">view img↗</a></figcaption>
                </figure>
                
                <p><i>"real enough to trigger instinct,"</i> highlighting how tone and timing directly affected NPC behavior. Faculty noted the platform's potential as both a recruitment and empathy-training tool.</p>
                
                <p>Performance optimization on Quest 3 brought stable frame rates and reduced motion fatigue, allowing longer, more natural sessions.</p>
                
                <p>Each scenario now runs with full voice acting, randomized NPCs, and tracked decision outcomes — setting the stage for scalable deployment to other departments.</p>
                
                <figure class="article-figure">
                    <video class="article-video" controls  autoplay muted loop>
                        <source src="vid/UE5 VR Simulation - UI Interaction, MetaHuman Character Selection & Level Portal.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <figcaption>Early prototyping: UI Interaction, MetaHuman Character Selection & Level Portal</figcaption>
                </figure>
            
            </section>


            <footer>
                <br><br><br>
                <hr><br>
                <div class="footer-container">
                <a href="#top" class="hyp-p">back to top ↑</a>
                </div>
            </footer>
        
        </article>
        
    </div>

    <script src="../../script.js"></script>
    <script src="../../shared.js"></script>
</body>
</html>
